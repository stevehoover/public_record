Each time a designer makes a logic change, the same simulations are repeated on a model that is 99.9% the same as it was before. The opportunity is to avoid repeating simulation steps that have already been performed. Even within a single simulation run, there tends to be a high degree of repetition. A software loop is a good example. Logic replication within a model is also a source of repetition. And even further, there is significant repetition across multiple simulations on the same model. As an extreme example, they all go through the same reset sequence.

With proper partitioning of logic functions, inputs and states can be captured that are likely to be repeated. Repeated inputs/states will produce identical outputs, and these outputs can be provided by the cache to avoid re-simulation. Cache entries can persist across runs as long as the associated logic is unchanged.

AI might be a new opportunity to assist in the partitioning task. Dynamic information and dynamic partitioning can identify optimal boundaries and improve caching performance. Timing-abstraction and transaction-level design embodied in TL-Verilog are forms of abstraction that can help to establish appropriate partitioning of logic (in time and space) that is likely to provide cache hits. Symbolic simulation can also increase the likelihood of cache hits significantly (and accelerate simulation in itself).

A simulation cache fundamentally reduces the amount of simulation performed, reducing simulation time without increasing simulation resources. While simulation steps are avoided, caching operations are added, and the benefit depends on a careful balancing act and creative heuristics. The potential benefit is difficult to predict. The opportunity is probably over 1000x, in theory, but more like 10x in practice, considering caching overheads.
